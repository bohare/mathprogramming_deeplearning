{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_np = np.array([[[1, 1, 1, 1, 1, 2],\n",
    "                 [1, 2, 3, 4, 0, 0],\n",
    "                 [1, 1, 0, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 1, 1, 1, 2],\n",
    "                [1, 2, 3, 4, 0, 0],\n",
    "                [0, 1, 1, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 1, 1, 1, 2],\n",
    "                [1, 2, 3, 4, 0, 0],\n",
    "                [1, 0, 1, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 1, 1, 1, 2],\n",
    "                [1, 2, 3, 4, 0, 0],\n",
    "                [1, 1, 1, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 1, 1, 1, 2],\n",
    "                [2, 1, 3, 4, 0, 0],\n",
    "                [1, 1, 0, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 1, 1, 1, 2],\n",
    "                [2, 1, 3, 4, 0, 0],\n",
    "                [1, 1, 1, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 1, 1, 1, 2],\n",
    "                [2, 1, 3, 5, 0, 0],\n",
    "                [1, 1, 1, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 1, 1, 1, 2],\n",
    "                [2, 1, 3, 5, 0, 0],\n",
    "                [0, 1, 1, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 1, 1, 1, 2],\n",
    "                [2, 1, 3, 5, 0, 0],\n",
    "                [1, 1, 0, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 1, 1, 1, 2],\n",
    "                [2, 1, 3, 5, 0, 0],\n",
    "                [1, 0, 1, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 1, 1, 1, 2],\n",
    "                [2, 1, 3, 6, 0, 0],\n",
    "                [1, 1, 0, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 1, 1, 1, 2],\n",
    "                [2, 1, 3, 6, 0, 0],\n",
    "                [1, 0, 1, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 1, 1, 1, 2],\n",
    "                [2, 1, 3, 6, 0, 0],\n",
    "                [0, 1, 1, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 1, 1, 1, 2],\n",
    "                [2, 1, 3, 6, 0, 0],\n",
    "                [1, 1, 1, 1, 0, 0]],\n",
    "               \n",
    "                [[1, 1, 1, 1, 1, 2],\n",
    "                [1, 3, 2, 3, 0, 0],\n",
    "                [1, 1, 0, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 1, 1, 1, 2],\n",
    "                [1, 3, 2, 3, 0, 0],\n",
    "                [0, 1, 1, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 1, 1, 1, 2],\n",
    "                [1, 3, 2, 3, 0, 0],\n",
    "                [1, 0, 1, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 1, 1, 1, 2],\n",
    "                [1, 3, 2, 3, 0, 0],\n",
    "                [1, 1, 1, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 1, 1, 1, 2],\n",
    "                [2, 1, 3, 3, 0, 0],\n",
    "                [1, 1, 0, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 1, 1, 1, 2],\n",
    "                [2, 1, 3, 3, 0, 0],\n",
    "                [1, 1, 1, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 1, 1, 2, 2],\n",
    "                [2, 1, 3, 3, 0, 0],\n",
    "                [1, 1, 0, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 1, 1, 2, 2],\n",
    "                [2, 1, 3, 3, 0, 0],\n",
    "                [1, 1, 1, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 1, 1, 2, 2],\n",
    "                [2, 1, 3, 3, 0, 0],\n",
    "                [0, 1, 1, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 1, 1, 2, 2],\n",
    "                [2, 1, 3, 3, 0, 0],\n",
    "                [1, 0, 1, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 1, 1, 2, 2],\n",
    "                [-2, -3, -4, -2, 0, 0],\n",
    "                [1, 1, 0, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 1, 1, 2, 2],\n",
    "                [-2, -3, -4, -9, 0, 0],\n",
    "                [0, 1, 1, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 1, 1, 2, 2],\n",
    "                [-2, -3, -4, -2, 0, 0],\n",
    "                [1, 0, 1, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 1, 1, 2, 2],\n",
    "                [-2, -3, -4, -2, 0, 0],\n",
    "                [1, 1, 1, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 1, 1, 2, 2],\n",
    "                [-2, -3, -4, -3, 0, 0],\n",
    "                [1, 1, 1, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 1, 1, 2, 2],\n",
    "                [-2, -3, -4, -3, 0, 0],\n",
    "                [1, 0, 1, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 1, 1, 2, 2],\n",
    "                [-2, -3, -6, -5, 0, 0],\n",
    "                [1, 1, 0, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 1, 1, 2, 2],\n",
    "                [-2, -3, -6, -5, 0, 0],\n",
    "                [0, 1, 1, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 1, 1, 2, 2],\n",
    "                [-2, -3, -6, -5, 0, 0],\n",
    "                [1, 0, 1, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 1, 1, 2, 2],\n",
    "                [-2, -3, -6, -5, 0, 0],\n",
    "                [1, 1, 1, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 1, 1, 2, 2],\n",
    "                [-3, -2, -6, -5, 0, 0],\n",
    "                [1, 1, 0, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 1, 1, 2, 2],\n",
    "                [-3, -2, -6, -5, 0, 0],\n",
    "                [0, 1, 1, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 1, 1, 2, 2],\n",
    "                [-3, -2, -6, -5, 0, 0],\n",
    "                [1, 0, 1, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 1, 1, 2, 2],\n",
    "                [-3, -2, -6, -5, 0, 0],\n",
    "                [1, 1, 1, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 1, 1, 2, 2],\n",
    "                [-3, -2, -6, -11, 0, 0],\n",
    "                [1, 1, 1, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 1, 1, 2, 2],\n",
    "                [-3, -2, -6, -11, 0, 0],\n",
    "                [1, 0, 1, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 2, 1, 1, 2],\n",
    "                [-3, -2, -6, -11, 0, 0],\n",
    "                [1, 1, 0, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 2, 1, 1, 2],\n",
    "                [2, 1, 3, 4, 0, 0],\n",
    "                [0, 1, 1, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 2, 1, 1, 2],\n",
    "                [2, 1, 3, 4, 0, 0],\n",
    "                [1, 0, 1, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 2, 1, 1, 2],\n",
    "                [2, 1, 3, 4, 0, 0],\n",
    "                [1, 1, 1, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 2, 1, 1, 2],\n",
    "                [3, 1, 2, 5, 0, 0],\n",
    "                [1, 1, 0, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 2, 1, 1, 2],\n",
    "                [3, 1, 2, 5, 0, 0],\n",
    "                [0, 1, 1, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 2, 1, 1, 2],\n",
    "                [3, 1, 2, 5, 0, 0],\n",
    "                [1, 0, 1, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 2, 1, 1, 2],\n",
    "                [3, 1, 2, 5, 0, 0],\n",
    "                [1, 1, 1, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 2, 1, 1, 2],\n",
    "                [1, 2, 3, 3, 0, 0],\n",
    "                [1, 1, 0, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 2, 1, 1, 2],\n",
    "                [1, 2, 3, 3, 0, 0],\n",
    "                [1, 1, 1, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 2, 1, 1, 2],\n",
    "                [3, 4, 1, 5, 0, 0],\n",
    "                [1, 1, 0, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 2, 1, 1, 2],\n",
    "                [3, 4, 1, 5, 0, 0],\n",
    "                [0, 1, 1, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 2, 1, 1, 2],\n",
    "                [3, 4, 1, 5, 0, 0],\n",
    "                [1, 0, 1, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 2, 1, 1, 2],\n",
    "                [3, 4, 1, 5, 0, 0],\n",
    "                [1, 1, 1, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 2, 1, 1, 2],\n",
    "                [4, 3, 5, 7, 0, 0],\n",
    "                [1, 1, 0, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 2, 1, 1, 2],\n",
    "                [4, 3, 5, 7, 0, 0],\n",
    "                [0, 1, 1, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 2, 1, 1, 2],\n",
    "                [4, 3, 5, 7, 0, 0],\n",
    "                [1, 0, 1, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 2, 1, 1, 2],\n",
    "                [4, 3, 5, 7, 0, 0],\n",
    "                [1, 1, 1, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 2, 1, 1, 2],\n",
    "                [2, 3, 1, 6, 0, 0],\n",
    "                [1, 0, 1, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 2, 1, 1, 2],\n",
    "                [2, 3, 1, 6, 0, 0],\n",
    "                [1, 1, 1, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 2, 1, 2, 2],\n",
    "                [2, 3, 1, 6, 0, 0],\n",
    "                [1, 1, 0, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 2, 1, 2, 2],\n",
    "                [2, 3, 1, 6, 0, 0],\n",
    "                [0, 1, 1, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 2, 1, 2, 2],\n",
    "                [2, 3, 1, 6, 0, 0],\n",
    "                [1, 0, 1, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 2, 1, 2, 2],\n",
    "                [2, 3, 1, 6, 0, 0],\n",
    "                [1, 1, 1, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 2, 1, 2, 2],\n",
    "                [1, 3, 2, 3, 0, 0],\n",
    "                [1, 1, 0, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 2, 1, 2, 2],\n",
    "                [1, 3, 2, 3, 0, 0],\n",
    "                [0, 1, 1, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 2, 1, 2, 2],\n",
    "                [1, 3, 2, 3, 0, 0],\n",
    "                [1, 0, 1, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 2, 1, 2, 2],\n",
    "                [1, 3, 2, 3, 0, 0],\n",
    "                [1, 1, 1, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 2, 1, 2, 2],\n",
    "                [1, 3, 2, 5, 0, 0],\n",
    "                [1, 0, 1, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 2, 1, 2, 2],\n",
    "                [1, 3, 2, 5, 0, 0],\n",
    "                [0, 1, 1, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 2, 1, 2, 2],\n",
    "                [2, 1, 3, 5, 0, 0],\n",
    "                [1, 1, 0, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 2, 1, 2, 2],\n",
    "                [2, 1, 3, 5, 0, 0],\n",
    "                [0, 1, 1, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 2, 1, 2, 2],\n",
    "                [2, 1, 3, 5, 0, 0],\n",
    "                [1, 0, 1, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 2, 1, 2, 2],\n",
    "                [2, 1, 3, 5, 0, 0],\n",
    "                [1, 1, 1, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 2, 1, 2, 2],\n",
    "                [-2, -3, -4, -5, 0, 0],\n",
    "                [1, 1, 0, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 2, 1, 2, 2],\n",
    "                [-2, -3, -4, -5, 0, 0],\n",
    "                [0, 1, 1, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 2, 1, 2, 2],\n",
    "                [-2, -3, -4, -5, 0, 0],\n",
    "                [1, 0, 1, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 2, 1, 2, 2],\n",
    "                [-2, -3, -4, -5, 0, 0],\n",
    "                [1, 1, 1, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 2, 1, 2, 2],\n",
    "                [-3, -2, -1, -6, 0, 0],\n",
    "                [1, 0, 1, 1, 0, 0]],\n",
    "                \n",
    "                [[1, 1, 2, 1, 2, 2],\n",
    "                [-3, -2, -1, -6, 0, 0],\n",
    "                [1, 1, 1, 1, 0, 0]],\n",
    "                \n",
    "                [[2, 1, 1, 1, 1, 2],\n",
    "                [-3, -2, -1, -6, 0, 0],\n",
    "                [1, 1, 0, 1, 0, 0]],\n",
    "                \n",
    "                [[2, 1, 1, 1, 1, 2],\n",
    "                [-3, -2, -1, -6, 0, 0],\n",
    "                [1, 1, 1, 1, 0, 0]],\n",
    "                \n",
    "                [[2, 1, 1, 1, 1, 2],\n",
    "                [3, 2, 1, 3, 0, 0],\n",
    "                [1, 1, 0, 1, 0, 0]],\n",
    "                \n",
    "                [[2, 1, 1, 1, 1, 2],\n",
    "                [3, 2, 1, 3, 0, 0],\n",
    "                [0, 1, 1, 1, 0, 0]],\n",
    "                \n",
    "                [[2, 1, 1, 1, 1, 2],\n",
    "                [3, 2, 1, 3, 0, 0],\n",
    "                [1, 0, 1, 1, 0, 0]],\n",
    "                \n",
    "                [[2, 1, 1, 1, 1, 2],\n",
    "                [3, 2, 1, 3, 0, 0],\n",
    "                [1, 1, 1, 1, 0, 0]],\n",
    "                \n",
    "                [[2, 1, 1, 1, 1, 2],\n",
    "                [2, 3, 1, 5, 0, 0],\n",
    "                [1, 1, 0, 1, 0, 0]],\n",
    "                \n",
    "                [[2, 1, 1, 1, 1, 2],\n",
    "                [2, 3, 1, 5, 0, 0],\n",
    "                [0, 1, 1, 1, 0, 0]],\n",
    "                \n",
    "                [[2, 1, 1, 1, 1, 2],\n",
    "                [2, 3, 1, 5, 0, 0],\n",
    "                [1, 0, 1, 1, 0, 0]],\n",
    "                \n",
    "                [[2, 1, 1, 1, 1, 2],\n",
    "                [2, 3, 1, 5, 0, 0],\n",
    "                [1, 1, 1, 1, 0, 0]],\n",
    "                \n",
    "                [[2, 1, 1, 1, 1, 2],\n",
    "                [1, 3, 2, 5, 0, 0],\n",
    "                [1, 1, 0, 1, 0, 0]],\n",
    "                \n",
    "                [[2, 1, 1, 1, 1, 2],\n",
    "                [1, 3, 2, 5, 0, 0],\n",
    "                [0, 1, 1, 1, 0, 0]],\n",
    "                \n",
    "                [[2, 1, 1, 1, 1, 2],\n",
    "                [1, 3, 2, 5, 0, 0],\n",
    "                [1, 0, 1, 1, 0, 0]],\n",
    "                \n",
    "                [[2, 1, 1, 1, 1, 2],\n",
    "                [1, 3, 2, 5, 0, 0],\n",
    "                [1, 1, 1, 1, 0, 0]],\n",
    "                \n",
    "                [[2, 1, 1, 1, 1, 2],\n",
    "                [1, 2, 3, 6, 0, 0],\n",
    "                [1, 1, 0, 1, 0, 0]],\n",
    "                \n",
    "                [[2, 1, 1, 1, 1, 2],\n",
    "                [1, 2, 3, 6, 0, 0],\n",
    "                [0, 1, 1, 1, 0, 0]],\n",
    "                \n",
    "                [[2, 1, 1, 1, 1, 2],\n",
    "                [1, 2, 3, 6, 0, 0],\n",
    "                [1, 0, 1, 1, 0, 0]],\n",
    "                 \n",
    "                [[2, 1, 1, 1, 1, 2],\n",
    "                [1, 2, 3, 6, 0, 0],\n",
    "                [1, 1, 1, 1, 0, 0]],\n",
    "                \n",
    "                [[2, 1, 1, 1, 1, 2],\n",
    "                [3, 2, 1, 5, 0, 0],\n",
    "                [1, 1, 0, 1, 0, 0]],\n",
    "                \n",
    "                [[2, 1, 1, 1, 1, 2],\n",
    "                [3, 2, 1, 5, 0, 0],\n",
    "                [1, 0, 1, 1, 0, 0]]])\n",
    "\n",
    "y_np = np.array([[3],\n",
    "                [3],\n",
    "                [3],\n",
    "                [3],\n",
    "                [3],\n",
    "                [3],\n",
    "                [5],\n",
    "                [5],\n",
    "                [5],\n",
    "                [5],\n",
    "                [7],\n",
    "                [7],\n",
    "                [7],\n",
    "                [7],\n",
    "                [2],\n",
    "                [2],\n",
    "                [3],\n",
    "                [2],\n",
    "                [3],\n",
    "                [1],\n",
    "                [5],\n",
    "                [1],\n",
    "                [1],\n",
    "                [1],\n",
    "                [4],\n",
    "                [1],\n",
    "                [4],\n",
    "                [4],\n",
    "                [4],\n",
    "                [4],\n",
    "                [4],\n",
    "                [2],\n",
    "                [4],\n",
    "                [4],\n",
    "                [4],\n",
    "                [2],\n",
    "                [4],\n",
    "                [4],\n",
    "                [4],\n",
    "                [4],\n",
    "                [4],\n",
    "                [5],\n",
    "                [5],\n",
    "                [5],\n",
    "                [5],\n",
    "                [5],\n",
    "                [5],\n",
    "                [5],\n",
    "                [6],\n",
    "                [1],\n",
    "                [6],\n",
    "                [6],\n",
    "                [6],\n",
    "                [6],\n",
    "                [6],\n",
    "                [6],\n",
    "                [6],\n",
    "                [6],\n",
    "                [7],\n",
    "                [7],\n",
    "                [7],\n",
    "                [7],\n",
    "                [7],\n",
    "                [7],\n",
    "                [2],\n",
    "                [2],\n",
    "                [6],\n",
    "                [2],\n",
    "                [3],\n",
    "                [3],\n",
    "                [5],\n",
    "                [5],\n",
    "                [5],\n",
    "                [5],\n",
    "                [4],\n",
    "                [2],\n",
    "                [4],\n",
    "                [4],\n",
    "                [4],\n",
    "                [4],\n",
    "                [4],\n",
    "                [1],\n",
    "                [4],\n",
    "                [5],\n",
    "                [4],\n",
    "                [4],\n",
    "                [6],\n",
    "                [6],\n",
    "                [6],\n",
    "                [6],\n",
    "                [3],\n",
    "                [3],\n",
    "                [3],\n",
    "                [3],\n",
    "                [7],\n",
    "                [7],\n",
    "                [7],\n",
    "                [7],\n",
    "                [6],\n",
    "                [6]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_np's pre-expansion shape: (100, 3, 6)\n",
      "y_np shape: (100, 1)\n",
      "X_np's shape after adding an extra dimension to conform it to have 4 dimensions: (100, 1, 3, 6)\n"
     ]
    }
   ],
   "source": [
    "print \"X_np's pre-expansion shape: {}\".format(X_np.shape)\n",
    "print \"y_np shape: {}\".format(y_np.shape)\n",
    "\n",
    "X_np = np.expand_dims(X_np, axis = 1)\n",
    "print \"X_np's shape after adding an extra dimension to conform it to have 4 dimensions: {}\".format(X_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 2, 2, 2, 2, 4, 4, 4, 4, 6, 6, 6, 6, 1, 1, 2, 1, 2, 0, 4, 0, 0, 0, 3]\n"
     ]
    }
   ],
   "source": [
    "def multi_class_format(numpy_array):\n",
    "    new = []\n",
    "    for item in y_np:\n",
    "        for i in item:\n",
    "            new.append(i)\n",
    "    \n",
    "    class_labels = []\n",
    "    \n",
    "    for i in new:\n",
    "        identifier = i-1\n",
    "        class_labels.append(identifier)\n",
    "        \n",
    "    return class_labels\n",
    "\n",
    "Y_array = multi_class_format(y_np)\n",
    "print Y_array[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_classes = 7 #Total number of classes an LP can belong to\n",
    "\n",
    "#Function to convert the output label vector into one hot format\n",
    "def get_one_hot(targets, nb_classes):\n",
    "    return np.eye(nb_classes)[np.array(targets).reshape(-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: (100, 7) \n",
      "[[ 0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "y_one_hot = get_one_hot(Y_array, 7)\n",
    "print \"Output shape: {} \".format(y_one_hot.shape)\n",
    "print y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: 80, y_train: 80 , X_test: 20 , y_test: 20 \n"
     ]
    }
   ],
   "source": [
    "# Make a train/test split using 20% test size\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_np, y_one_hot, test_size=0.20, shuffle = True)\n",
    "print \"X_train: {}, y_train: {} , X_test: {} , y_test: {} \".format(X_train.shape[0], y_train.shape[0], X_test.shape[0], y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: torch.Size([80, 1, 3, 6]) \n",
      "Training labels shape: torch.Size([80, 7]) \n",
      "Test set shape: torch.Size([20, 1, 3, 6]) \n",
      "Test labels shape: torch.Size([20, 7]) \n"
     ]
    }
   ],
   "source": [
    "X_train = torch.from_numpy(X_train).type(torch.FloatTensor)\n",
    "y_train = torch.from_numpy(y_train).type(torch.LongTensor)\n",
    "\n",
    "X_test = torch.from_numpy(X_test).type(torch.FloatTensor)\n",
    "y_test = torch.from_numpy(y_test).type(torch.LongTensor)\n",
    "\n",
    "print \"Training set shape: {} \".format(X_train.size())\n",
    "print \"Training labels shape: {} \".format(y_train.size())\n",
    "\n",
    "print \"Test set shape: {} \".format(X_test.size())\n",
    "print \"Test labels shape: {} \".format(y_test.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Class for CNN with 3 filters of size 3 each\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = 1, out_channels = 3, kernel_size=3, stride = 1, padding = 1, dilation = 1, bias = True),\n",
    "            nn.ReLU())\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = 3, out_channels = 3, kernel_size=3, stride = 1, padding = 1, dilation = 1, bias = True),\n",
    "            nn.ReLU())\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = 3, out_channels = 3, kernel_size=3, stride = 1, padding = 1, dilation = 1, bias = True),\n",
    "            nn.ReLU())\n",
    "        self.fc1 = nn.Conv2d(in_channels = 3, out_channels = 7, kernel_size = 1, stride = 1, padding = 0, dilation = 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.fc1(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "#Class for CNN with 2 filters of size 3 each\n",
    "class CNN2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN2, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = 1, out_channels = 2, kernel_size=3, stride = 1, padding = 1, dilation = 1, bias = True),\n",
    "            nn.ReLU())\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = 2, out_channels = 2, kernel_size=3, stride = 1, padding = 1, dilation = 1, bias = True),\n",
    "            nn.ReLU())\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = 2, out_channels = 2, kernel_size=3, stride = 1, padding = 1, dilation = 1, bias = True),\n",
    "            nn.ReLU())\n",
    "        self.fc1 = nn.Conv2d(in_channels = 2, out_channels = 7, kernel_size = 1, stride = 1, padding = 0, dilation = 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.fc1(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN2(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d (1, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d (2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Conv2d (2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (fc1): Conv2d (2, 7, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "cnn = CNN2()\n",
    "print cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Converting the training one hot vector labels into indices because PyTorch does not take one-hot vectors\n",
    "targetnp = y_train.numpy()\n",
    "idxs = np.where(targetnp > 0)[1]\n",
    "#d_out = np.reshape(idxs, (4,3,7))\n",
    "new_targets = torch.LongTensor(idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Converting the test one hot vector labels into indices\n",
    "test_targetnp = y_test.numpy()\n",
    "test_idxs = np.where(test_targetnp > 0)[1]\n",
    "new_test_targets = torch.LongTensor(test_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a Adam optimizer\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=learning_rate, weight_decay = 0)\n",
    "# create a loss function\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Epoch: 0 Average loss: 5.6784, Training Accuracy: 0/80 (0.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 10 Average loss: 5.6213, Training Accuracy: 0/80 (0.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 20 Average loss: 5.5705, Training Accuracy: 0/80 (0.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 30 Average loss: 5.5216, Training Accuracy: 0/80 (0.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 40 Average loss: 5.4699, Training Accuracy: 0/80 (0.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 50 Average loss: 5.4098, Training Accuracy: 0/80 (0.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 60 Average loss: 5.3272, Training Accuracy: 0/80 (0.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 70 Average loss: 5.2189, Training Accuracy: 0/80 (0.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 80 Average loss: 5.0890, Training Accuracy: 0/80 (0.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 90 Average loss: 4.9435, Training Accuracy: 0/80 (0.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 100 Average loss: 4.7766, Training Accuracy: 0/80 (0.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 110 Average loss: 4.5904, Training Accuracy: 0/80 (0.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 120 Average loss: 4.3967, Training Accuracy: 0/80 (0.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 130 Average loss: 4.1999, Training Accuracy: 2/80 (2.5000%)\n",
      "\n",
      "\n",
      "Training Epoch: 140 Average loss: 4.0058, Training Accuracy: 12/80 (15.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 150 Average loss: 3.8252, Training Accuracy: 14/80 (17.5000%)\n",
      "\n",
      "\n",
      "Training Epoch: 160 Average loss: 3.6585, Training Accuracy: 14/80 (17.5000%)\n",
      "\n",
      "\n",
      "Training Epoch: 170 Average loss: 3.5060, Training Accuracy: 16/80 (20.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 180 Average loss: 3.3638, Training Accuracy: 17/80 (21.2500%)\n",
      "\n",
      "\n",
      "Training Epoch: 190 Average loss: 3.2313, Training Accuracy: 19/80 (23.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 200 Average loss: 3.1097, Training Accuracy: 22/80 (27.5000%)\n",
      "\n",
      "\n",
      "Training Epoch: 210 Average loss: 3.0007, Training Accuracy: 21/80 (26.2500%)\n",
      "\n",
      "\n",
      "Training Epoch: 220 Average loss: 2.9056, Training Accuracy: 21/80 (26.2500%)\n",
      "\n",
      "\n",
      "Training Epoch: 230 Average loss: 2.8229, Training Accuracy: 21/80 (26.2500%)\n",
      "\n",
      "\n",
      "Training Epoch: 240 Average loss: 2.7523, Training Accuracy: 18/80 (22.5000%)\n",
      "\n",
      "\n",
      "Training Epoch: 250 Average loss: 2.6937, Training Accuracy: 18/80 (22.5000%)\n",
      "\n",
      "\n",
      "Training Epoch: 260 Average loss: 2.6451, Training Accuracy: 20/80 (25.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 270 Average loss: 2.6046, Training Accuracy: 20/80 (25.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 280 Average loss: 2.5703, Training Accuracy: 20/80 (25.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 290 Average loss: 2.5404, Training Accuracy: 22/80 (27.5000%)\n",
      "\n",
      "\n",
      "Training Epoch: 300 Average loss: 2.5141, Training Accuracy: 23/80 (28.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 310 Average loss: 2.4905, Training Accuracy: 23/80 (28.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 320 Average loss: 2.4690, Training Accuracy: 23/80 (28.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 330 Average loss: 2.4490, Training Accuracy: 24/80 (30.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 340 Average loss: 2.4303, Training Accuracy: 23/80 (28.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 350 Average loss: 2.4124, Training Accuracy: 23/80 (28.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 360 Average loss: 2.3950, Training Accuracy: 23/80 (28.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 370 Average loss: 2.3781, Training Accuracy: 23/80 (28.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 380 Average loss: 2.3613, Training Accuracy: 24/80 (30.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 390 Average loss: 2.3446, Training Accuracy: 24/80 (30.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 400 Average loss: 2.3278, Training Accuracy: 24/80 (30.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 410 Average loss: 2.3108, Training Accuracy: 24/80 (30.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 420 Average loss: 2.2936, Training Accuracy: 24/80 (30.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 430 Average loss: 2.2763, Training Accuracy: 23/80 (28.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 440 Average loss: 2.2586, Training Accuracy: 23/80 (28.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 450 Average loss: 2.2407, Training Accuracy: 23/80 (28.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 460 Average loss: 2.2225, Training Accuracy: 23/80 (28.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 470 Average loss: 2.2042, Training Accuracy: 23/80 (28.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 480 Average loss: 2.1857, Training Accuracy: 23/80 (28.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 490 Average loss: 2.1672, Training Accuracy: 23/80 (28.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 500 Average loss: 2.1486, Training Accuracy: 23/80 (28.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 510 Average loss: 2.1300, Training Accuracy: 23/80 (28.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 520 Average loss: 2.1115, Training Accuracy: 23/80 (28.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 530 Average loss: 2.0932, Training Accuracy: 23/80 (28.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 540 Average loss: 2.0751, Training Accuracy: 23/80 (28.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 550 Average loss: 2.0573, Training Accuracy: 23/80 (28.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 560 Average loss: 2.0402, Training Accuracy: 24/80 (30.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 570 Average loss: 2.0237, Training Accuracy: 24/80 (30.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 580 Average loss: 2.0080, Training Accuracy: 24/80 (30.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 590 Average loss: 1.9932, Training Accuracy: 24/80 (30.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 600 Average loss: 1.9795, Training Accuracy: 24/80 (30.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 610 Average loss: 1.9668, Training Accuracy: 24/80 (30.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 620 Average loss: 1.9551, Training Accuracy: 24/80 (30.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 630 Average loss: 1.9448, Training Accuracy: 24/80 (30.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 640 Average loss: 1.9353, Training Accuracy: 24/80 (30.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 650 Average loss: 1.9268, Training Accuracy: 23/80 (28.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 660 Average loss: 1.9189, Training Accuracy: 23/80 (28.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 670 Average loss: 1.9117, Training Accuracy: 23/80 (28.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 680 Average loss: 1.9050, Training Accuracy: 22/80 (27.5000%)\n",
      "\n",
      "\n",
      "Training Epoch: 690 Average loss: 1.8989, Training Accuracy: 22/80 (27.5000%)\n",
      "\n",
      "\n",
      "Training Epoch: 700 Average loss: 1.8933, Training Accuracy: 22/80 (27.5000%)\n",
      "\n",
      "\n",
      "Training Epoch: 710 Average loss: 1.8881, Training Accuracy: 22/80 (27.5000%)\n",
      "\n",
      "\n",
      "Training Epoch: 720 Average loss: 1.8834, Training Accuracy: 21/80 (26.2500%)\n",
      "\n",
      "\n",
      "Training Epoch: 730 Average loss: 1.8793, Training Accuracy: 21/80 (26.2500%)\n",
      "\n",
      "\n",
      "Training Epoch: 740 Average loss: 1.8756, Training Accuracy: 21/80 (26.2500%)\n",
      "\n",
      "\n",
      "Training Epoch: 750 Average loss: 1.8721, Training Accuracy: 21/80 (26.2500%)\n",
      "\n",
      "\n",
      "Training Epoch: 760 Average loss: 1.8689, Training Accuracy: 21/80 (26.2500%)\n",
      "\n",
      "\n",
      "Training Epoch: 770 Average loss: 1.8658, Training Accuracy: 21/80 (26.2500%)\n",
      "\n",
      "\n",
      "Training Epoch: 780 Average loss: 1.8630, Training Accuracy: 21/80 (26.2500%)\n",
      "\n",
      "\n",
      "Training Epoch: 790 Average loss: 1.8602, Training Accuracy: 21/80 (26.2500%)\n",
      "\n",
      "\n",
      "Training Epoch: 800 Average loss: 1.8577, Training Accuracy: 21/80 (26.2500%)\n",
      "\n",
      "\n",
      "Training Epoch: 810 Average loss: 1.8552, Training Accuracy: 21/80 (26.2500%)\n",
      "\n",
      "\n",
      "Training Epoch: 820 Average loss: 1.8529, Training Accuracy: 21/80 (26.2500%)\n",
      "\n",
      "\n",
      "Training Epoch: 830 Average loss: 1.8507, Training Accuracy: 21/80 (26.2500%)\n",
      "\n",
      "\n",
      "Training Epoch: 840 Average loss: 1.8486, Training Accuracy: 23/80 (28.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 850 Average loss: 1.8465, Training Accuracy: 23/80 (28.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 860 Average loss: 1.8445, Training Accuracy: 23/80 (28.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 870 Average loss: 1.8425, Training Accuracy: 23/80 (28.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 880 Average loss: 1.8406, Training Accuracy: 23/80 (28.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 890 Average loss: 1.8388, Training Accuracy: 23/80 (28.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 900 Average loss: 1.8370, Training Accuracy: 23/80 (28.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 910 Average loss: 1.8353, Training Accuracy: 23/80 (28.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 920 Average loss: 1.8335, Training Accuracy: 23/80 (28.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 930 Average loss: 1.8319, Training Accuracy: 23/80 (28.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 940 Average loss: 1.8302, Training Accuracy: 23/80 (28.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 950 Average loss: 1.8286, Training Accuracy: 23/80 (28.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 960 Average loss: 1.8269, Training Accuracy: 23/80 (28.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 970 Average loss: 1.8253, Training Accuracy: 23/80 (28.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 980 Average loss: 1.8237, Training Accuracy: 23/80 (28.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 990 Average loss: 1.8222, Training Accuracy: 23/80 (28.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 1000 Average loss: 1.8206, Training Accuracy: 23/80 (28.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 1010 Average loss: 1.8190, Training Accuracy: 23/80 (28.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 1020 Average loss: 1.8175, Training Accuracy: 23/80 (28.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 1030 Average loss: 1.8160, Training Accuracy: 23/80 (28.7500%)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Epoch: 1040 Average loss: 1.8145, Training Accuracy: 23/80 (28.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 1050 Average loss: 1.8130, Training Accuracy: 23/80 (28.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 1060 Average loss: 1.8115, Training Accuracy: 22/80 (27.5000%)\n",
      "\n",
      "\n",
      "Training Epoch: 1070 Average loss: 1.8101, Training Accuracy: 22/80 (27.5000%)\n",
      "\n",
      "\n",
      "Training Epoch: 1080 Average loss: 1.8086, Training Accuracy: 22/80 (27.5000%)\n",
      "\n",
      "\n",
      "Training Epoch: 1090 Average loss: 1.8072, Training Accuracy: 22/80 (27.5000%)\n",
      "\n",
      "\n",
      "Training Epoch: 1100 Average loss: 1.8058, Training Accuracy: 22/80 (27.5000%)\n",
      "\n",
      "\n",
      "Training Epoch: 1110 Average loss: 1.8044, Training Accuracy: 22/80 (27.5000%)\n",
      "\n",
      "\n",
      "Training Epoch: 1120 Average loss: 1.8029, Training Accuracy: 22/80 (27.5000%)\n",
      "\n",
      "\n",
      "Training Epoch: 1130 Average loss: 1.8015, Training Accuracy: 22/80 (27.5000%)\n",
      "\n",
      "\n",
      "Training Epoch: 1140 Average loss: 1.8001, Training Accuracy: 22/80 (27.5000%)\n",
      "\n",
      "\n",
      "Training Epoch: 1150 Average loss: 1.7987, Training Accuracy: 22/80 (27.5000%)\n",
      "\n",
      "\n",
      "Training Epoch: 1160 Average loss: 1.7973, Training Accuracy: 22/80 (27.5000%)\n",
      "\n",
      "\n",
      "Training Epoch: 1170 Average loss: 1.7960, Training Accuracy: 24/80 (30.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 1180 Average loss: 1.7946, Training Accuracy: 24/80 (30.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 1190 Average loss: 1.7932, Training Accuracy: 24/80 (30.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 1200 Average loss: 1.7918, Training Accuracy: 24/80 (30.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 1210 Average loss: 1.7904, Training Accuracy: 24/80 (30.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 1220 Average loss: 1.7891, Training Accuracy: 24/80 (30.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 1230 Average loss: 1.7875, Training Accuracy: 24/80 (30.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 1240 Average loss: 1.7859, Training Accuracy: 23/80 (28.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 1250 Average loss: 1.7843, Training Accuracy: 23/80 (28.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 1260 Average loss: 1.7827, Training Accuracy: 23/80 (28.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 1270 Average loss: 1.7811, Training Accuracy: 23/80 (28.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 1280 Average loss: 1.7795, Training Accuracy: 23/80 (28.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 1290 Average loss: 1.7779, Training Accuracy: 23/80 (28.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 1300 Average loss: 1.7763, Training Accuracy: 23/80 (28.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 1310 Average loss: 1.7747, Training Accuracy: 23/80 (28.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 1320 Average loss: 1.7732, Training Accuracy: 23/80 (28.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 1330 Average loss: 1.7717, Training Accuracy: 23/80 (28.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 1340 Average loss: 1.7702, Training Accuracy: 23/80 (28.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 1350 Average loss: 1.7688, Training Accuracy: 24/80 (30.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 1360 Average loss: 1.7673, Training Accuracy: 24/80 (30.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 1370 Average loss: 1.7659, Training Accuracy: 24/80 (30.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 1380 Average loss: 1.7644, Training Accuracy: 24/80 (30.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 1390 Average loss: 1.7630, Training Accuracy: 24/80 (30.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 1400 Average loss: 1.7615, Training Accuracy: 24/80 (30.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 1410 Average loss: 1.7601, Training Accuracy: 24/80 (30.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 1420 Average loss: 1.7587, Training Accuracy: 25/80 (31.2500%)\n",
      "\n",
      "\n",
      "Training Epoch: 1430 Average loss: 1.7573, Training Accuracy: 25/80 (31.2500%)\n",
      "\n",
      "\n",
      "Training Epoch: 1440 Average loss: 1.7558, Training Accuracy: 25/80 (31.2500%)\n",
      "\n",
      "\n",
      "Training Epoch: 1450 Average loss: 1.7544, Training Accuracy: 25/80 (31.2500%)\n",
      "\n",
      "\n",
      "Training Epoch: 1460 Average loss: 1.7530, Training Accuracy: 25/80 (31.2500%)\n",
      "\n",
      "\n",
      "Training Epoch: 1470 Average loss: 1.7516, Training Accuracy: 25/80 (31.2500%)\n",
      "\n",
      "\n",
      "Training Epoch: 1480 Average loss: 1.7502, Training Accuracy: 25/80 (31.2500%)\n",
      "\n",
      "\n",
      "Training Epoch: 1490 Average loss: 1.7488, Training Accuracy: 25/80 (31.2500%)\n",
      "\n",
      "\n",
      "Training Epoch: 1500 Average loss: 1.7474, Training Accuracy: 25/80 (31.2500%)\n",
      "\n",
      "\n",
      "Training Epoch: 1510 Average loss: 1.7460, Training Accuracy: 25/80 (31.2500%)\n",
      "\n",
      "\n",
      "Training Epoch: 1520 Average loss: 1.7446, Training Accuracy: 26/80 (32.5000%)\n",
      "\n",
      "\n",
      "Training Epoch: 1530 Average loss: 1.7432, Training Accuracy: 26/80 (32.5000%)\n",
      "\n",
      "\n",
      "Training Epoch: 1540 Average loss: 1.7418, Training Accuracy: 26/80 (32.5000%)\n",
      "\n",
      "\n",
      "Training Epoch: 1550 Average loss: 1.7404, Training Accuracy: 26/80 (32.5000%)\n",
      "\n",
      "\n",
      "Training Epoch: 1560 Average loss: 1.7391, Training Accuracy: 26/80 (32.5000%)\n",
      "\n",
      "\n",
      "Training Epoch: 1570 Average loss: 1.7377, Training Accuracy: 26/80 (32.5000%)\n",
      "\n",
      "\n",
      "Training Epoch: 1580 Average loss: 1.7363, Training Accuracy: 26/80 (32.5000%)\n",
      "\n",
      "\n",
      "Training Epoch: 1590 Average loss: 1.7349, Training Accuracy: 26/80 (32.5000%)\n",
      "\n",
      "\n",
      "Training Epoch: 1600 Average loss: 1.7335, Training Accuracy: 26/80 (32.5000%)\n",
      "\n",
      "\n",
      "Training Epoch: 1610 Average loss: 1.7321, Training Accuracy: 26/80 (32.5000%)\n",
      "\n",
      "\n",
      "Training Epoch: 1620 Average loss: 1.7306, Training Accuracy: 27/80 (33.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 1630 Average loss: 1.7290, Training Accuracy: 27/80 (33.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 1640 Average loss: 1.7271, Training Accuracy: 27/80 (33.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 1650 Average loss: 1.7253, Training Accuracy: 27/80 (33.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 1660 Average loss: 1.7238, Training Accuracy: 28/80 (35.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 1670 Average loss: 1.7223, Training Accuracy: 28/80 (35.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 1680 Average loss: 1.7208, Training Accuracy: 28/80 (35.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 1690 Average loss: 1.7192, Training Accuracy: 28/80 (35.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 1700 Average loss: 1.7172, Training Accuracy: 28/80 (35.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 1710 Average loss: 1.7154, Training Accuracy: 28/80 (35.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 1720 Average loss: 1.7142, Training Accuracy: 28/80 (35.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 1730 Average loss: 1.7129, Training Accuracy: 28/80 (35.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 1740 Average loss: 1.7116, Training Accuracy: 28/80 (35.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 1750 Average loss: 1.7104, Training Accuracy: 29/80 (36.2500%)\n",
      "\n",
      "\n",
      "Training Epoch: 1760 Average loss: 1.7091, Training Accuracy: 29/80 (36.2500%)\n",
      "\n",
      "\n",
      "Training Epoch: 1770 Average loss: 1.7079, Training Accuracy: 29/80 (36.2500%)\n",
      "\n",
      "\n",
      "Training Epoch: 1780 Average loss: 1.7067, Training Accuracy: 29/80 (36.2500%)\n",
      "\n",
      "\n",
      "Training Epoch: 1790 Average loss: 1.7054, Training Accuracy: 29/80 (36.2500%)\n",
      "\n",
      "\n",
      "Training Epoch: 1800 Average loss: 1.7042, Training Accuracy: 29/80 (36.2500%)\n",
      "\n",
      "\n",
      "Training Epoch: 1810 Average loss: 1.7030, Training Accuracy: 29/80 (36.2500%)\n",
      "\n",
      "\n",
      "Training Epoch: 1820 Average loss: 1.7018, Training Accuracy: 29/80 (36.2500%)\n",
      "\n",
      "\n",
      "Training Epoch: 1830 Average loss: 1.7006, Training Accuracy: 29/80 (36.2500%)\n",
      "\n",
      "\n",
      "Training Epoch: 1840 Average loss: 1.6994, Training Accuracy: 29/80 (36.2500%)\n",
      "\n",
      "\n",
      "Training Epoch: 1850 Average loss: 1.6981, Training Accuracy: 29/80 (36.2500%)\n",
      "\n",
      "\n",
      "Training Epoch: 1860 Average loss: 1.6969, Training Accuracy: 29/80 (36.2500%)\n",
      "\n",
      "\n",
      "Training Epoch: 1870 Average loss: 1.6958, Training Accuracy: 30/80 (37.5000%)\n",
      "\n",
      "\n",
      "Training Epoch: 1880 Average loss: 1.6946, Training Accuracy: 30/80 (37.5000%)\n",
      "\n",
      "\n",
      "Training Epoch: 1890 Average loss: 1.6934, Training Accuracy: 30/80 (37.5000%)\n",
      "\n",
      "\n",
      "Training Epoch: 1900 Average loss: 1.6922, Training Accuracy: 30/80 (37.5000%)\n",
      "\n",
      "\n",
      "Training Epoch: 1910 Average loss: 1.6910, Training Accuracy: 30/80 (37.5000%)\n",
      "\n",
      "\n",
      "Training Epoch: 1920 Average loss: 1.6898, Training Accuracy: 30/80 (37.5000%)\n",
      "\n",
      "\n",
      "Training Epoch: 1930 Average loss: 1.6886, Training Accuracy: 30/80 (37.5000%)\n",
      "\n",
      "\n",
      "Training Epoch: 1940 Average loss: 1.6874, Training Accuracy: 30/80 (37.5000%)\n",
      "\n",
      "\n",
      "Training Epoch: 1950 Average loss: 1.6862, Training Accuracy: 30/80 (37.5000%)\n",
      "\n",
      "\n",
      "Training Epoch: 1960 Average loss: 1.6850, Training Accuracy: 30/80 (37.5000%)\n",
      "\n",
      "\n",
      "Training Epoch: 1970 Average loss: 1.6839, Training Accuracy: 30/80 (37.5000%)\n",
      "\n",
      "\n",
      "Training Epoch: 1980 Average loss: 1.6827, Training Accuracy: 29/80 (36.2500%)\n",
      "\n",
      "\n",
      "Training Epoch: 1990 Average loss: 1.6815, Training Accuracy: 26/80 (32.5000%)\n",
      "\n",
      "\n",
      "Training Epoch: 2000 Average loss: 1.6803, Training Accuracy: 26/80 (32.5000%)\n",
      "\n",
      "\n",
      "Training Epoch: 2010 Average loss: 1.6792, Training Accuracy: 26/80 (32.5000%)\n",
      "\n",
      "\n",
      "Training Epoch: 2020 Average loss: 1.6780, Training Accuracy: 26/80 (32.5000%)\n",
      "\n",
      "\n",
      "Training Epoch: 2030 Average loss: 1.6769, Training Accuracy: 26/80 (32.5000%)\n",
      "\n",
      "\n",
      "Training Epoch: 2040 Average loss: 1.6757, Training Accuracy: 26/80 (32.5000%)\n",
      "\n",
      "\n",
      "Training Epoch: 2050 Average loss: 1.6746, Training Accuracy: 26/80 (32.5000%)\n",
      "\n",
      "\n",
      "Training Epoch: 2060 Average loss: 1.6734, Training Accuracy: 26/80 (32.5000%)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Epoch: 2070 Average loss: 1.6723, Training Accuracy: 26/80 (32.5000%)\n",
      "\n",
      "\n",
      "Training Epoch: 2080 Average loss: 1.6711, Training Accuracy: 26/80 (32.5000%)\n",
      "\n",
      "\n",
      "Training Epoch: 2090 Average loss: 1.6700, Training Accuracy: 27/80 (33.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 2100 Average loss: 1.6688, Training Accuracy: 27/80 (33.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 2110 Average loss: 1.6676, Training Accuracy: 27/80 (33.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 2120 Average loss: 1.6664, Training Accuracy: 27/80 (33.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 2130 Average loss: 1.6652, Training Accuracy: 27/80 (33.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 2140 Average loss: 1.6640, Training Accuracy: 27/80 (33.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 2150 Average loss: 1.6626, Training Accuracy: 27/80 (33.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 2160 Average loss: 1.6612, Training Accuracy: 27/80 (33.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 2170 Average loss: 1.6598, Training Accuracy: 27/80 (33.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 2180 Average loss: 1.6584, Training Accuracy: 27/80 (33.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 2190 Average loss: 1.6571, Training Accuracy: 27/80 (33.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 2200 Average loss: 1.6558, Training Accuracy: 27/80 (33.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 2210 Average loss: 1.6545, Training Accuracy: 27/80 (33.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 2220 Average loss: 1.6533, Training Accuracy: 28/80 (35.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 2230 Average loss: 1.6520, Training Accuracy: 29/80 (36.2500%)\n",
      "\n",
      "\n",
      "Training Epoch: 2240 Average loss: 1.6508, Training Accuracy: 29/80 (36.2500%)\n",
      "\n",
      "\n",
      "Training Epoch: 2250 Average loss: 1.6496, Training Accuracy: 29/80 (36.2500%)\n",
      "\n",
      "\n",
      "Training Epoch: 2260 Average loss: 1.6484, Training Accuracy: 30/80 (37.5000%)\n",
      "\n",
      "\n",
      "Training Epoch: 2270 Average loss: 1.6472, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 2280 Average loss: 1.6461, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 2290 Average loss: 1.6450, Training Accuracy: 32/80 (40.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 2300 Average loss: 1.6438, Training Accuracy: 32/80 (40.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 2310 Average loss: 1.6427, Training Accuracy: 32/80 (40.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 2320 Average loss: 1.6416, Training Accuracy: 32/80 (40.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 2330 Average loss: 1.6405, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 2340 Average loss: 1.6393, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 2350 Average loss: 1.6383, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 2360 Average loss: 1.6372, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 2370 Average loss: 1.6361, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 2380 Average loss: 1.6351, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 2390 Average loss: 1.6341, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 2400 Average loss: 1.6330, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 2410 Average loss: 1.6320, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 2420 Average loss: 1.6310, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 2430 Average loss: 1.6301, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 2440 Average loss: 1.6291, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 2450 Average loss: 1.6281, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 2460 Average loss: 1.6272, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 2470 Average loss: 1.6262, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 2480 Average loss: 1.6253, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 2490 Average loss: 1.6244, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 2500 Average loss: 1.6235, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 2510 Average loss: 1.6225, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 2520 Average loss: 1.6216, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 2530 Average loss: 1.6208, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 2540 Average loss: 1.6199, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 2550 Average loss: 1.6190, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 2560 Average loss: 1.6181, Training Accuracy: 32/80 (40.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 2570 Average loss: 1.6173, Training Accuracy: 32/80 (40.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 2580 Average loss: 1.6164, Training Accuracy: 32/80 (40.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 2590 Average loss: 1.6155, Training Accuracy: 32/80 (40.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 2600 Average loss: 1.6147, Training Accuracy: 32/80 (40.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 2610 Average loss: 1.6138, Training Accuracy: 32/80 (40.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 2620 Average loss: 1.6130, Training Accuracy: 32/80 (40.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 2630 Average loss: 1.6121, Training Accuracy: 32/80 (40.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 2640 Average loss: 1.6113, Training Accuracy: 32/80 (40.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 2650 Average loss: 1.6104, Training Accuracy: 32/80 (40.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 2660 Average loss: 1.6096, Training Accuracy: 32/80 (40.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 2670 Average loss: 1.6087, Training Accuracy: 32/80 (40.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 2680 Average loss: 1.6079, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 2690 Average loss: 1.6070, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 2700 Average loss: 1.6062, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 2710 Average loss: 1.6054, Training Accuracy: 32/80 (40.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 2720 Average loss: 1.6045, Training Accuracy: 32/80 (40.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 2730 Average loss: 1.6037, Training Accuracy: 32/80 (40.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 2740 Average loss: 1.6028, Training Accuracy: 32/80 (40.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 2750 Average loss: 1.6020, Training Accuracy: 32/80 (40.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 2760 Average loss: 1.6011, Training Accuracy: 32/80 (40.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 2770 Average loss: 1.6003, Training Accuracy: 32/80 (40.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 2780 Average loss: 1.5994, Training Accuracy: 32/80 (40.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 2790 Average loss: 1.5985, Training Accuracy: 32/80 (40.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 2800 Average loss: 1.5976, Training Accuracy: 32/80 (40.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 2810 Average loss: 1.5968, Training Accuracy: 32/80 (40.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 2820 Average loss: 1.5958, Training Accuracy: 32/80 (40.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 2830 Average loss: 1.5950, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 2840 Average loss: 1.5941, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 2850 Average loss: 1.5932, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 2860 Average loss: 1.5923, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 2870 Average loss: 1.5914, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 2880 Average loss: 1.5905, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 2890 Average loss: 1.5896, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 2900 Average loss: 1.5886, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 2910 Average loss: 1.5877, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 2920 Average loss: 1.5867, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 2930 Average loss: 1.5858, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 2940 Average loss: 1.5849, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 2950 Average loss: 1.5839, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 2960 Average loss: 1.5830, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 2970 Average loss: 1.5820, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 2980 Average loss: 1.5810, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 2990 Average loss: 1.5801, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 3000 Average loss: 1.5791, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 3010 Average loss: 1.5781, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 3020 Average loss: 1.5771, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 3030 Average loss: 1.5761, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 3040 Average loss: 1.5751, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 3050 Average loss: 1.5741, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 3060 Average loss: 1.5731, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 3070 Average loss: 1.5721, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 3080 Average loss: 1.5711, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 3090 Average loss: 1.5701, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 3100 Average loss: 1.5691, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 3110 Average loss: 1.5680, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 3120 Average loss: 1.5670, Training Accuracy: 32/80 (40.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 3130 Average loss: 1.5660, Training Accuracy: 32/80 (40.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 3140 Average loss: 1.5650, Training Accuracy: 32/80 (40.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 3150 Average loss: 1.5640, Training Accuracy: 32/80 (40.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 3160 Average loss: 1.5629, Training Accuracy: 32/80 (40.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 3170 Average loss: 1.5620, Training Accuracy: 32/80 (40.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 3180 Average loss: 1.5609, Training Accuracy: 32/80 (40.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 3190 Average loss: 1.5599, Training Accuracy: 32/80 (40.0000%)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Epoch: 3200 Average loss: 1.5589, Training Accuracy: 32/80 (40.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 3210 Average loss: 1.5579, Training Accuracy: 32/80 (40.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 3220 Average loss: 1.5569, Training Accuracy: 32/80 (40.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 3230 Average loss: 1.5559, Training Accuracy: 32/80 (40.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 3240 Average loss: 1.5549, Training Accuracy: 32/80 (40.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 3250 Average loss: 1.5536, Training Accuracy: 32/80 (40.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 3260 Average loss: 1.5522, Training Accuracy: 32/80 (40.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 3270 Average loss: 1.5508, Training Accuracy: 32/80 (40.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 3280 Average loss: 1.5495, Training Accuracy: 32/80 (40.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 3290 Average loss: 1.5482, Training Accuracy: 32/80 (40.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 3300 Average loss: 1.5470, Training Accuracy: 32/80 (40.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 3310 Average loss: 1.5458, Training Accuracy: 32/80 (40.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 3320 Average loss: 1.5448, Training Accuracy: 32/80 (40.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 3330 Average loss: 1.5436, Training Accuracy: 32/80 (40.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 3340 Average loss: 1.5426, Training Accuracy: 32/80 (40.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 3350 Average loss: 1.5415, Training Accuracy: 32/80 (40.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 3360 Average loss: 1.5404, Training Accuracy: 32/80 (40.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 3370 Average loss: 1.5394, Training Accuracy: 32/80 (40.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 3380 Average loss: 1.5383, Training Accuracy: 32/80 (40.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 3390 Average loss: 1.5373, Training Accuracy: 32/80 (40.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 3400 Average loss: 1.5363, Training Accuracy: 32/80 (40.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 3410 Average loss: 1.5353, Training Accuracy: 32/80 (40.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 3420 Average loss: 1.5343, Training Accuracy: 32/80 (40.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 3430 Average loss: 1.5333, Training Accuracy: 32/80 (40.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 3440 Average loss: 1.5323, Training Accuracy: 32/80 (40.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 3450 Average loss: 1.5313, Training Accuracy: 32/80 (40.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 3460 Average loss: 1.5303, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 3470 Average loss: 1.5294, Training Accuracy: 32/80 (40.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 3480 Average loss: 1.5285, Training Accuracy: 32/80 (40.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 3490 Average loss: 1.5277, Training Accuracy: 32/80 (40.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 3500 Average loss: 1.5268, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 3510 Average loss: 1.5260, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 3520 Average loss: 1.5252, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 3530 Average loss: 1.5243, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 3540 Average loss: 1.5235, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 3550 Average loss: 1.5228, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 3560 Average loss: 1.5220, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 3570 Average loss: 1.5212, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 3580 Average loss: 1.5205, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 3590 Average loss: 1.5198, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 3600 Average loss: 1.5190, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 3610 Average loss: 1.5183, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 3620 Average loss: 1.5176, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 3630 Average loss: 1.5169, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 3640 Average loss: 1.5162, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 3650 Average loss: 1.5156, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 3660 Average loss: 1.5149, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 3670 Average loss: 1.5142, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 3680 Average loss: 1.5135, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 3690 Average loss: 1.5129, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 3700 Average loss: 1.5123, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 3710 Average loss: 1.5116, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 3720 Average loss: 1.5110, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 3730 Average loss: 1.5104, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 3740 Average loss: 1.5097, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 3750 Average loss: 1.5091, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 3760 Average loss: 1.5085, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 3770 Average loss: 1.5079, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 3780 Average loss: 1.5072, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 3790 Average loss: 1.5066, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 3800 Average loss: 1.5060, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 3810 Average loss: 1.5054, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 3820 Average loss: 1.5048, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 3830 Average loss: 1.5042, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 3840 Average loss: 1.5036, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 3850 Average loss: 1.5030, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 3860 Average loss: 1.5023, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 3870 Average loss: 1.5015, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 3880 Average loss: 1.5008, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 3890 Average loss: 1.5001, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 3900 Average loss: 1.4993, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 3910 Average loss: 1.4986, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 3920 Average loss: 1.4979, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 3930 Average loss: 1.4972, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 3940 Average loss: 1.4966, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 3950 Average loss: 1.4959, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 3960 Average loss: 1.4952, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 3970 Average loss: 1.4945, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 3980 Average loss: 1.4938, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 3990 Average loss: 1.4931, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4000 Average loss: 1.4924, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4010 Average loss: 1.4918, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4020 Average loss: 1.4911, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4030 Average loss: 1.4904, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4040 Average loss: 1.4897, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4050 Average loss: 1.4891, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4060 Average loss: 1.4884, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4070 Average loss: 1.4878, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4080 Average loss: 1.4871, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4090 Average loss: 1.4862, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4100 Average loss: 1.4856, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4110 Average loss: 1.4849, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4120 Average loss: 1.4843, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4130 Average loss: 1.4836, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4140 Average loss: 1.4829, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4150 Average loss: 1.4822, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4160 Average loss: 1.4816, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4170 Average loss: 1.4809, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4180 Average loss: 1.4802, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4190 Average loss: 1.4796, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4200 Average loss: 1.4789, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4210 Average loss: 1.4783, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4220 Average loss: 1.4776, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4230 Average loss: 1.4769, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4240 Average loss: 1.4763, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4250 Average loss: 1.4757, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4260 Average loss: 1.4750, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4270 Average loss: 1.4743, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4280 Average loss: 1.4738, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4290 Average loss: 1.4731, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4300 Average loss: 1.4724, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4310 Average loss: 1.4718, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4320 Average loss: 1.4711, Training Accuracy: 31/80 (38.7500%)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Epoch: 4330 Average loss: 1.4705, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4340 Average loss: 1.4699, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4350 Average loss: 1.4692, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4360 Average loss: 1.4685, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4370 Average loss: 1.4679, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4380 Average loss: 1.4672, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4390 Average loss: 1.4666, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4400 Average loss: 1.4659, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4410 Average loss: 1.4653, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4420 Average loss: 1.4646, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4430 Average loss: 1.4640, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4440 Average loss: 1.4632, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4450 Average loss: 1.4626, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4460 Average loss: 1.4619, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4470 Average loss: 1.4613, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4480 Average loss: 1.4606, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4490 Average loss: 1.4599, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4500 Average loss: 1.4592, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4510 Average loss: 1.4586, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4520 Average loss: 1.4579, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4530 Average loss: 1.4572, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4540 Average loss: 1.4565, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4550 Average loss: 1.4558, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4560 Average loss: 1.4552, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4570 Average loss: 1.4545, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4580 Average loss: 1.4539, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4590 Average loss: 1.4532, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4600 Average loss: 1.4526, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4610 Average loss: 1.4519, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4620 Average loss: 1.4512, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4630 Average loss: 1.4505, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4640 Average loss: 1.4498, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4650 Average loss: 1.4492, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4660 Average loss: 1.4485, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4670 Average loss: 1.4478, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4680 Average loss: 1.4471, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4690 Average loss: 1.4463, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4700 Average loss: 1.4456, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4710 Average loss: 1.4449, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4720 Average loss: 1.4442, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4730 Average loss: 1.4435, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4740 Average loss: 1.4425, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4750 Average loss: 1.4412, Training Accuracy: 30/80 (37.5000%)\n",
      "\n",
      "\n",
      "Training Epoch: 4760 Average loss: 1.4399, Training Accuracy: 30/80 (37.5000%)\n",
      "\n",
      "\n",
      "Training Epoch: 4770 Average loss: 1.4386, Training Accuracy: 30/80 (37.5000%)\n",
      "\n",
      "\n",
      "Training Epoch: 4780 Average loss: 1.4357, Training Accuracy: 29/80 (36.2500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4790 Average loss: 1.4337, Training Accuracy: 29/80 (36.2500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4800 Average loss: 1.4305, Training Accuracy: 29/80 (36.2500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4810 Average loss: 1.4272, Training Accuracy: 29/80 (36.2500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4820 Average loss: 1.4243, Training Accuracy: 30/80 (37.5000%)\n",
      "\n",
      "\n",
      "Training Epoch: 4830 Average loss: 1.4218, Training Accuracy: 32/80 (40.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 4840 Average loss: 1.4195, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4850 Average loss: 1.4174, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4860 Average loss: 1.4155, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4870 Average loss: 1.4137, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4880 Average loss: 1.4120, Training Accuracy: 31/80 (38.7500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4890 Average loss: 1.4105, Training Accuracy: 32/80 (40.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 4900 Average loss: 1.4091, Training Accuracy: 32/80 (40.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 4910 Average loss: 1.4077, Training Accuracy: 32/80 (40.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 4920 Average loss: 1.4064, Training Accuracy: 32/80 (40.0000%)\n",
      "\n",
      "\n",
      "Training Epoch: 4930 Average loss: 1.4052, Training Accuracy: 33/80 (41.2500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4940 Average loss: 1.4040, Training Accuracy: 33/80 (41.2500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4950 Average loss: 1.4028, Training Accuracy: 33/80 (41.2500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4960 Average loss: 1.4017, Training Accuracy: 33/80 (41.2500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4970 Average loss: 1.4005, Training Accuracy: 33/80 (41.2500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4980 Average loss: 1.3994, Training Accuracy: 33/80 (41.2500%)\n",
      "\n",
      "\n",
      "Training Epoch: 4990 Average loss: 1.3982, Training Accuracy: 35/80 (43.7500%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run the main training loop\n",
    "epochs = 5000\n",
    "train_loss = 0\n",
    "train_correct = 0\n",
    "test_correct = 0\n",
    "for epoch in range(epochs):\n",
    "        data, target = Variable(X_train), Variable(new_targets)\n",
    "        optimizer.zero_grad()\n",
    "        net_out = cnn(data)\n",
    "        #print net_out\n",
    "        #print target.data\n",
    "        \n",
    "        #sum up the loss\n",
    "        train_loss = loss_func(net_out, target)\n",
    "        pred = net_out.data.max(1)[1] #get the index of the max log probabitlity\n",
    "        train_correct = pred.eq(target.data).sum()\n",
    "        \n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 10 == 0:\n",
    "            print('\\nTraining Epoch: {} Average loss: {:.4f}, Training Accuracy: {}/{} ({:.4f}%)\\n'.format(\n",
    "        epoch, train_loss.data[0], train_correct, len(target),\n",
    "        100. * train_correct / len(target)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test loss: 1.5167, Test accuracy: 8/20 (40.0000%)\n",
      "\n",
      "Time taken to detect:  0.000662088394165\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0\n",
    "test_correct = 0\n",
    "\n",
    "#Converting test data and target labels into pyTorch variables\n",
    "test_data, test_target = Variable(X_test), Variable(new_test_targets)\n",
    "\n",
    "tic = time.time()\n",
    "#Test set output\n",
    "net_out = cnn(test_data)\n",
    "\n",
    "test_loss = loss_func(net_out, test_target)\n",
    "test_pred = net_out.data.max(1)[1] #get the index of max log probability\n",
    "test_correct = test_pred.eq(test_target.data).sum()\n",
    "\n",
    "toc = time.time()\n",
    "time_diff = toc - tic\n",
    "\n",
    "print('\\nTest loss: {:.4f}, Test accuracy: {}/{} ({:.4f}%)\\n'.format(\n",
    "    test_loss.data[0], test_correct, len(test_target), 100. * test_correct / len(test_target)))\n",
    "\n",
    "print \"Time taken to detect: \", time_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
